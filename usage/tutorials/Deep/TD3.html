

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Twin Delayed DDPG &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Soft Actor-Critic" href="SAC.html" />
    <link rel="prev" title="Deep Deterministic Policy Gradients" href="DDPG.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../bandit/index.html">Bandit Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deep RL Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="Background.html">Deep Reinforcement Learning Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="VPG.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="A2C.html">Advantage Actor Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="PPO.html">Proximal Policy Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="DQN.html">Deep Q-Networks (DQN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="Double_DQN.html">Double Deep Q-Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="Dueling_DQN.html">Dueling Deep Q-Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="NoisyNet_DQN.html">Deep Q Networks with Noisy Nets</a></li>
<li class="toctree-l3"><a class="reference internal" href="Prioritized_DQN.html">Prioritized Deep Q-Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="DDPG.html">Deep Deterministic Policy Gradients</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Twin Delayed DDPG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#objective">Objective</a></li>
<li class="toctree-l4"><a class="reference internal" href="#algorithm-details">Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-through-the-api">Training through the API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="SAC.html">Soft Actor-Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html">Categorical Deep Q-Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#objective">Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#distributional-bellman">Distributional Bellman</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#algorithm-details">Algorithm Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#training-through-the-api">Training through the API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20shared%20parameters%20in%20actor%20critic%20agents.html">Using Shared Parameters in Actor Critic Agents in GenRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Saving%20and%20loading.html">Saving and Loading Weights and Hyperparameters with GenRL</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Deep RL Tutorials</a> &raquo;</li>
        
      <li>Twin Delayed DDPG</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/Deep/TD3.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="twin-delayed-ddpg">
<h1>Twin Delayed DDPG<a class="headerlink" href="#twin-delayed-ddpg" title="Permalink to this headline">¶</a></h1>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>Similar to Deep Q-Networks, the problem of overestimation of the state values, occuring due to noisy function approximators and using the same function approximator for action selection and value estimation also persists in actor-critic
algorithms with continuous action-spaces. Double DQN, the solution for this problem in Deep Q-Networks is not effective in actor-critic algorithms due to the slow rate of change of the policy. Twin Delayed DDPG (TD3) uses <em>Clipped Double Q-Learning</em> to
address this problem. TD3 uses two Q function approximators and the loss function for each is given by</p>
<div class="math notranslate nohighlight">
\[L(\phi_{1}, \mathcal{D}) = E_{(s,a,r,s',d) \sim \mathcal{D}}[(Q_{\phi_{1}}(s, a) - y(r,s',d))^2]\]</div>
<div class="math notranslate nohighlight">
\[L(\phi_{2}, \mathcal{D}) = E_{(s,a,r,s',d) \sim \mathcal{D}}[(Q_{\phi_{2}}(s, a) - y(r,s',d))^2]\]</div>
</div>
<div class="section" id="algorithm-details">
<h2>Algorithm Details<a class="headerlink" href="#algorithm-details" title="Permalink to this headline">¶</a></h2>
<div class="section" id="clipped-double-q-learning">
<h3>Clipped Double Q-Learning<a class="headerlink" href="#clipped-double-q-learning" title="Permalink to this headline">¶</a></h3>
<p>Double DQNs are not effective in actor-critic algorithms due to the slow change in the policy and the original double Q-Learning (although being somewhat effective) does not completely solve the problem of overestimation. To tackle this TD3 uses <em>Clipped Double Q-Learning</em>
Clipped Double Q-Learning proposes to upper bound the less biased critic network by the more biased one and hence no additional overestimation can be introdiced. Although, this may introduce underestimation, it is not much of a concern since underestimation errors don’t propagate
through policy updates. The target function calculated usign Clipped Double Q-Learning for the updates can be written as</p>
<div class="math notranslate nohighlight">
\[y = r + \gamma min_{i=1,2}Q_{\theta_i'}(s', \pi_{\phi_1}(s'))\]</div>
<p>Both of the critic networks are updated using the loss functions mentioned above.</p>
</div>
<div class="section" id="experience-replay">
<h3>Experience Replay<a class="headerlink" href="#experience-replay" title="Permalink to this headline">¶</a></h3>
<p>TD3 being an off-policy algorithm, makes use of <em>Replay Buffer</em>. Whenever a transition <span class="math notranslate nohighlight">\((s_t, a_t, r_t, s_{t+1})\)</span> is encountered, it is stored into the replay buffer. Batches of these transitions are
sampled while updating the network parameters. This helps in breaking the strong correlation between the updates that would have been present had the transitions been trained and discarded immediately after they are encountered
and also helps to avoid the rapid forgetting of the possibly rare transitions that would be useful later on.</p>
<div class="highlight-default notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104</pre></div></td><td class="code"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Helper function to log</span>

<span class="sd">        Sends useful parameters to the logger.</span>

<span class="sd">        Args:</span>
<span class="sd">            timestep (int): Current timestep of training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;timestep&quot;</span><span class="p">:</span> <span class="n">timestep</span><span class="p">,</span>
                <span class="s2">&quot;Episode&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span>
                <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">get_logging_params</span><span class="p">(),</span>
                <span class="s2">&quot;Episode Reward&quot;</span><span class="p">:</span> <span class="n">safe_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_rewards</span><span class="p">),</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="target-policy-smoothing-regularization">
<h3>Target Policy Smoothing Regularization<a class="headerlink" href="#target-policy-smoothing-regularization" title="Permalink to this headline">¶</a></h3>
<p>TD3 adds noise to the target action to reduce the variance induced by function approximation error. This acts as a form of regularization which smoothens the changes in the action-values along changes in action</p>
<div class="math notranslate nohighlight">
\[a = \pi_{\phi'}(s') + \epsilon\]</div>
<div class="math notranslate nohighlight">
\[\epsilon \sim clip(\mathcal{N}(0, \sigma), -c, c)\]</div>
</div>
<div class="section" id="delayed-policy-updates">
<h3>Delayed Policy updates<a class="headerlink" href="#delayed-policy-updates" title="Permalink to this headline">¶</a></h3>
<p>TD3 uses target networks similar to DDPG and DQNs for the two critics and the actors to stabilise learning. Apart from this, it also promotes updating the policy networks at a lower frequency as compared to the Q-networks to avoid divergent behaviour for the policy. The paper
recommends one policy update for every two Q-function updates.</p>
<div class="highlight-default notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121</pre></div></td><td class="code"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update parameters of the model</span>

<span class="sd">        Args:</span>
<span class="sd">            update_interval (int): Interval between successive updates of the target model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">timestep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">update_interval</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_buffer</span><span class="p">()</span>

            <span class="n">value_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_q_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_value</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">value_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_value</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Delayed Update</span>
            <span class="k">if</span> <span class="n">timestep</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">policy_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_p_loss</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_policy</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">policy_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_policy</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;policy_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;value_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">update_target_model</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
<div class="section" id="training-through-the-api">
<h2>Training through the API<a class="headerlink" href="#training-through-the-api" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">genrl.agents</span> <span class="kn">import</span> <span class="n">TD3</span>
<span class="kn">from</span> <span class="nn">genrl.environments</span> <span class="kn">import</span> <span class="n">VectorEnv</span>
<span class="kn">from</span> <span class="nn">genrl.trainers</span> <span class="kn">import</span> <span class="n">OffPolicyTrainer</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">VectorEnv</span><span class="p">(</span><span class="s2">&quot;MountainCarContinuous-v0&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">TD3</span><span class="p">(</span><span class="s2">&quot;mlp&quot;</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">OffPolicyTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_timesteps</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="SAC.html" class="btn btn-neutral float-right" title="Soft Actor-Critic" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="DDPG.html" class="btn btn-neutral float-left" title="Deep Deterministic Policy Gradients" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>