

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deep Q Networks with Noisy Nets &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Prioritized Deep Q-Networks" href="Prioritized_DQN.html" />
    <link rel="prev" title="Dueling Deep Q-Network" href="Dueling_DQN.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../bandit/index.html">Bandit Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deep RL Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="Background.html">Deep Reinforcement Learning Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="VPG.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="A2C.html">Advantage Actor Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="PPO.html">Proximal Policy Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="DQN.html">Deep Q-Networks (DQN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="Double_DQN.html">Double Deep Q-Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="Dueling_DQN.html">Dueling Deep Q-Network</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Deep Q Networks with Noisy Nets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#objective">Objective</a></li>
<li class="toctree-l4"><a class="reference internal" href="#algorithm-details">Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-through-the-api">Training through the API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Prioritized_DQN.html">Prioritized Deep Q-Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="DDPG.html">Deep Deterministic Policy Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="TD3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l3"><a class="reference internal" href="SAC.html">Soft Actor-Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html">Categorical Deep Q-Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#objective">Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#distributional-bellman">Distributional Bellman</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#algorithm-details">Algorithm Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html#training-through-the-api">Training through the API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20shared%20parameters%20in%20actor%20critic%20agents.html">Using Shared Parameters in Actor Critic Agents in GenRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Saving%20and%20loading.html">Saving and Loading Weights and Hyperparameters with GenRL</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Deep RL Tutorials</a> &raquo;</li>
        
      <li>Deep Q Networks with Noisy Nets</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/Deep/NoisyNet_DQN.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-q-networks-with-noisy-nets">
<h1>Deep Q Networks with Noisy Nets<a class="headerlink" href="#deep-q-networks-with-noisy-nets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>NoisyNet DQN is a variant of DQN which uses fully connected layers with noisy parameters to drive exploration. Thus, the parametrised action-value function can now be seen as a random variable. The new loss function which
needs to minimised is defined as:</p>
<div class="math notranslate nohighlight">
\[E[E_{(x, a, r, y) \sim D}[r + \gamma max_{b \in A} Q(y, b, \epsilon'; \zeta^{-}) - Q(x, a, \epsilon; \zeta)]^{2}]\]</div>
<p>where <span class="math notranslate nohighlight">\(\zeta\)</span> is a set of learnable parameters for the noise.</p>
</div>
<div class="section" id="algorithm-details">
<h2>Algorithm Details<a class="headerlink" href="#algorithm-details" title="Permalink to this headline">¶</a></h2>
<div class="section" id="action-selection">
<h3>Action Selection<a class="headerlink" href="#action-selection" title="Permalink to this headline">¶</a></h3>
<p>The action selection is no longer epsilon-greedy since the exploration is driven by the noise in the neural network layers. The action selection is done greedily.</p>
</div>
<div class="section" id="noisy-parameters">
<h3>Noisy Parameters<a class="headerlink" href="#noisy-parameters" title="Permalink to this headline">¶</a></h3>
<p>A noisy parameter <span class="math notranslate nohighlight">\(\theta\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\theta \coloneqq \mu + \Sigma \odot \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span> are vectors of trainable parameters and <span class="math notranslate nohighlight">\(\epsilon\)</span> is a vector of zero mean noise. Hence, the loss function is now defined with respect to <span class="math notranslate nohighlight">\(\Sigma\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span>
and the optimization now takes place with respect to <span class="math notranslate nohighlight">\(\Sigma\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span>. <span class="math notranslate nohighlight">\(\epsilon\)</span> is sampled from factorised gaussian noise.</p>
</div>
<div class="section" id="experience-replay">
<h3>Experience Replay<a class="headerlink" href="#experience-replay" title="Permalink to this headline">¶</a></h3>
<p>Every transition occuring during the training is stored in a separate <em>Replay Buffer</em></p>
<div class="highlight-default notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104</pre></div></td><td class="code"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Helper function to log</span>

<span class="sd">        Sends useful parameters to the logger.</span>

<span class="sd">        Args:</span>
<span class="sd">            timestep (int): Current timestep of training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;timestep&quot;</span><span class="p">:</span> <span class="n">timestep</span><span class="p">,</span>
                <span class="s2">&quot;Episode&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span>
                <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">get_logging_params</span><span class="p">(),</span>
                <span class="s2">&quot;Episode Reward&quot;</span><span class="p">:</span> <span class="n">safe_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_rewards</span><span class="p">),</span>
</pre></div>
</td></tr></table></div>
<p>The transitions are later sampled in batches from the replay buffer for updating the network</p>
</div>
<div class="section" id="update-the-q-network">
<h3>Update the Q-Network<a class="headerlink" href="#update-the-q-network" title="Permalink to this headline">¶</a></h3>
<p>Once enough number of transitions ae stored in the replay buffer, we start updating the Q-values according to the given objective. The loss function is defined in a fashion similar to a DQN. This allows
any new improvisations in training techniques of DQN such as Double DQN or NoisyNet DQN to be readily adapted in the dueling architechture.</p>
<div class="highlight-default notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181</pre></div></td><td class="code"><div class="highlight"><pre><span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">timestep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">n_envs</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">update_params_before_select_action</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">render</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

            <span class="c1"># true_dones contains the &quot;true&quot; value of the dones (game over statuses). It is set</span>
            <span class="c1"># to False when the environment is not actually done but instead reaches the max</span>
            <span class="c1"># episode length.</span>
            <span class="n">true_dones</span> <span class="o">=</span> <span class="p">[</span><span class="n">info</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;done&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">n_envs</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">push</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">true_dones</span><span class="p">))</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_game_over_status</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">noise_reset</span><span class="p">()</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">timestep</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_update</span> <span class="ow">and</span> <span class="n">timestep</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">update_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_interval</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">timestep</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_update</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">!=</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="n">timestep</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
<div class="section" id="training-through-the-api">
<h2>Training through the API<a class="headerlink" href="#training-through-the-api" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">genrl.agents</span> <span class="kn">import</span> <span class="n">NoisyDQN</span>
<span class="kn">from</span> <span class="nn">genrl.environments</span> <span class="kn">import</span> <span class="n">VectorEnv</span>
<span class="kn">from</span> <span class="nn">genrl.trainers</span> <span class="kn">import</span> <span class="n">OffPolicyTrainer</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">VectorEnv</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">NoisyDQN</span><span class="p">(</span><span class="s2">&quot;mlp&quot;</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">OffPolicyTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_timesteps</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Prioritized_DQN.html" class="btn btn-neutral float-right" title="Prioritized Deep Q-Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Dueling_DQN.html" class="btn btn-neutral float-left" title="Dueling Deep Q-Network" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>